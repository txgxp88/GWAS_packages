{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93dc679-293f-4749-9408-6f0822c8746d",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#list all files from bulk:\n",
    "import dxpy\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db53f8e-288a-425e-b310-4ad8740bbd65",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Download_path\n",
    "# Step 1: Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current directory: {current_directory}\")\n",
    "\n",
    "# Step 2: Define the name of the new folder\n",
    "folder_name = \"File_DownLoad\"  # Change this to the name you want\n",
    "\n",
    "saving_dir = current_directory+'/'+folder_name\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.mkdir(folder_name)  # Create the directory\n",
    "    print(f\"Folder created: {saving_dir}\")\n",
    "else:\n",
    "    print(f\"Folder already exists: {saving_dir}\")\n",
    "    \n",
    "    \n",
    "# Step 3: Cloud goal \n",
    "phenotype_origin_folder = \"/GWAS_pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1687f1-885c-434f-8437-dea8161da960",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Freesurfer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f8472-a40e-4658-b1c4-1d2fab6e95ca",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run the command and capture the output\n",
    "FREESURFER_HOME = subprocess.run('echo $FREESURFER_HOME', shell=True, check=True, capture_output=True, text=True)\n",
    "\n",
    "# Get the stdout and strip any extra whitespace or newlines\n",
    "freesurfer_path = FREESURFER_HOME.stdout.strip()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Freesurfer path is: {freesurfer_path}\")\n",
    "\n",
    "#copy the template to saving path\n",
    "copy_command = f'cp -a {freesurfer_path}/subjects/fsaverage {saving_dir}'\n",
    "subprocess.run(copy_command, shell=True, check=True)\n",
    "\n",
    "#download annotate files\n",
    "project_id = 'project-GxqpVq0Jpp5Py82xVbZV198y'  # Replace with the actual project ID\n",
    "command = f'dx download \"{project_id}:/{phenotype_origin_folder}/Freesurfer_related/mask_lh.annot\" -o {saving_dir}'\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "command = f'dx download \"{project_id}:/{phenotype_origin_folder}/Freesurfer_related/mask_rh.annot\" -o {saving_dir}'\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "command = f'dx download \"{project_id}:/{phenotype_origin_folder}/Freesurfer_related/license.txt\" -o {saving_dir}'\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "\n",
    "#copy the licence to freesurfer path for usage\n",
    "copy_command = f'cp {saving_dir}/license.txt {freesurfer_path}'\n",
    "subprocess.run(copy_command, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aeb381-cddd-4a20-8fef-358e35033b75",
   "metadata": {},
   "source": [
    "# Environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f818b45-de0e-40ea-8d55-ca314521d511",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Environment setting\n",
    "\n",
    "os.environ['FREESURFER_HOME'] = freesurfer_path\n",
    "os.environ['SUBJECTS_DIR'] = saving_dir+'/'\n",
    "\n",
    "print(f\"freesurfer path exists: {freesurfer_path}\")\n",
    "print(f\"subject path exists: {saving_dir}\")\n",
    "\n",
    "# Source the setting up\n",
    "subprocess.run(freesurfer_path+'/sources.sh',shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175f520-7d5d-4298-8cbf-9295d056face",
   "metadata": {},
   "source": [
    "# Surf transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a11aab9-2cfe-46c4-9dc3-52e9421d18b5",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def surf_transform(file_name,subj_path,freesurfer_path):\n",
    "\n",
    "    # template\n",
    "    sfile_annot_lh = subj_path+'/mask_lh.annot'\n",
    "    tfile_annot_lh = subj_path+'/'+file_name+'_lh.annot'\n",
    "    \n",
    "    sfile_annot_rh = subj_path+'/mask_rh.annot'\n",
    "    tfile_annot_rh = subj_path+'/'+file_name+'_rh.annot'\n",
    "    \n",
    "    #################\n",
    "    command_annot2annot_lh = [freesurfer_path+'/bin/'+'mri_surf2surf',\n",
    "                        '--srcsubject','fsaverage',\n",
    "                        '--trgsubject',file_name,\n",
    "                        '--sval-annot',sfile_annot_lh,\n",
    "                        '--trgsurfval',tfile_annot_lh,\n",
    "                        '--hemi','lh']\n",
    "    \n",
    "    print(f\"surf transform in lh: {command_annot2annot_lh}\")\n",
    "    # subprocess.run(command_annot2annot_lh, shell=True)\n",
    "    # try:\n",
    "    #     result = subprocess.run(command_annot2annot_lh, shell=True, capture_output=True, text=True)\n",
    "    #     print(\"Command executed successfully\")\n",
    "    #     print(\"Output:\", result.stdout)\n",
    "    # except subprocess.CalledProcessError as e:\n",
    "    #     print(\"Command failed with exit status\", e.returncode)\n",
    "    #     print(\"Error output:\", e.stderr)\n",
    "    \n",
    "    command_annot2annot_rh = [freesurfer_path+'/bin/'+'mri_surf2surf',\n",
    "                        '--srcsubject','fsaverage',\n",
    "                        '--trgsubject',file_name,\n",
    "                        '--sval-annot',sfile_annot_rh,\n",
    "                        '--trgsurfval',tfile_annot_rh,\n",
    "                        '--hemi','rh']\n",
    "    \n",
    "    print(f\"surf transform in rh: {command_annot2annot_rh}\")\n",
    "    \n",
    "    \n",
    "    return command_annot2annot_lh, command_annot2annot_rh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90de44e-bc83-443c-bcbf-f9a3c741e4a8",
   "metadata": {},
   "source": [
    "# Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f79c95-2ef8-4499-808d-e03edc37f977",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def data_information_extraction(information, template):\n",
    "    \n",
    "    value_all_average = {}\n",
    "    value_all_sum = {}\n",
    "    \n",
    "    for region, value in template.items():\n",
    "        \n",
    "        # print(f\"Region: {region}, Value: {value}\")\n",
    "        \n",
    "        value_all_average[region] = np.average( information[value] )\n",
    "        value_all_sum[region] = np.sum( information[value] )\n",
    "        \n",
    "    return value_all_average, value_all_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f5112-82f4-486f-944a-64ce896aef29",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Read the files\n",
    "\n",
    "def data_loading(saving_dir, file_name):\n",
    "    \n",
    "    ## Annotate files: template\n",
    "    annot_data_lh = os.path.join( saving_dir, file_name[0:2]+'_'+file_name+'_lh.annot' )\n",
    "    annot_data_lh = nib.freesurfer.io.read_annot( annot_data_lh )\n",
    "    annot_data_lh = annot_data_lh[0]\n",
    "    \n",
    "    annot_data_rh = os.path.join( saving_dir, file_name[0:2]+'_'+file_name+'_rh.annot' )\n",
    "    annot_data_rh = nib.freesurfer.io.read_annot( annot_data_rh )\n",
    "    annot_data_rh = annot_data_rh[0]\n",
    "    \n",
    "    annot_data_lh_values = np.unique(annot_data_lh)\n",
    "    annot_data_rh_values = np.unique(annot_data_rh)\n",
    "\n",
    "    indices_dict_lh = {value: np.where(annot_data_lh == value)[0] for value in annot_data_lh_values}\n",
    "    indices_dict_rh = {value: np.where(annot_data_rh == value)[0] for value in annot_data_rh_values}\n",
    "    \n",
    "    # print(indices_dict_lh)\n",
    "    # print(indices_dict_rh)\n",
    "    \n",
    "    ## Mapping the values in different field\n",
    "    file_destination = os.path.join( saving_dir, file_name, 'surf' )\n",
    "    \n",
    "    # Area (average for each region)\n",
    "    if os.path.exists(file_destination+'/lh.area'):\n",
    "        area_data_lh = nib.freesurfer.io.read_morph_data( file_destination+'/lh.area' )\n",
    "        area_data_rh = nib.freesurfer.io.read_morph_data( file_destination+'/rh.area' )\n",
    "        \n",
    "        # print(area_data_lh)\n",
    "        \n",
    "        area_data_lh_template_average, area_data_lh_template_sum = data_information_extraction(area_data_lh, indices_dict_lh)\n",
    "        area_data_rh_template_average, area_data_rh_template_sum = data_information_extraction(area_data_rh, indices_dict_rh)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        area_data_lh_template_average = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        area_data_lh_template_sum = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        area_data_rh_template_average = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        area_data_rh_template_sum = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        \n",
    "    \n",
    "    #Curvature\n",
    "    if os.path.exists(file_destination+'/lh.curv'):\n",
    "        curve_data_lh = nib.freesurfer.io.read_morph_data( file_destination+'/lh.curv' )\n",
    "        curve_data_rh = nib.freesurfer.io.read_morph_data( file_destination+'/rh.curv' )\n",
    "\n",
    "        # print(curve_data_lh)\n",
    "\n",
    "        curve_data_lh_template_average, curve_data_lh_template_sum = data_information_extraction(curve_data_lh, indices_dict_lh)\n",
    "        curve_data_rh_template_average, curve_data_rh_template_sum = data_information_extraction(curve_data_rh, indices_dict_rh)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        curve_data_lh_template_average = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        curve_data_lh_template_sum = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        curve_data_rh_template_average = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        curve_data_rh_template_sum = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "\n",
    "    # thickness (average for each region)\n",
    "    if os.path.exists(file_destination+'/lh.thickness'):\n",
    "    \n",
    "        thickness_data_lh = nib.freesurfer.io.read_morph_data( file_destination+'/lh.thickness' )\n",
    "        thickness_data_rh = nib.freesurfer.io.read_morph_data( file_destination+'/rh.thickness' )\n",
    "\n",
    "        # print(thickness_data_lh)\n",
    "\n",
    "        thickness_data_lh_template_average, thickness_data_lh_template_sum = data_information_extraction(thickness_data_lh, indices_dict_lh)\n",
    "        thickness_data_rh_template_average, thickness_data_rh_template_sum = data_information_extraction(thickness_data_rh, indices_dict_rh)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        thickness_data_lh_template_average = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        thickness_data_lh_template_sum = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        thickness_data_rh_template_average = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        thickness_data_rh_template_sum = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        \n",
    "\n",
    "    # Total Volumn:\n",
    "    if os.path.exists(file_destination+'/lh.volume'):\n",
    "    \n",
    "        volumn_data_lh = nib.freesurfer.io.read_morph_data( file_destination+'/lh.volume' )\n",
    "        volumn_data_rh = nib.freesurfer.io.read_morph_data( file_destination+'/rh.volume' )\n",
    "\n",
    "        # print(volumn_data_lh)\n",
    "\n",
    "        volumn_data_lh_template_average, volumn_data_lh_template_sum = data_information_extraction(volumn_data_lh, indices_dict_lh)\n",
    "        volumn_data_rh_template_average, volumn_data_rh_template_sum = data_information_extraction(volumn_data_rh, indices_dict_rh)\n",
    "    \n",
    "    else:\n",
    "        volumn_data_lh_template_average = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        volumn_data_lh_template_sum = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        volumn_data_rh_template_average = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "        volumn_data_rh_template_sum = {i: np.nan for i in range(-1, 12) if i != 0}\n",
    "\n",
    "    \n",
    "    #### combine them as a frame\n",
    "    df_lh = pd.DataFrame([area_data_lh_template_average, curve_data_lh_template_average, thickness_data_lh_template_average, volumn_data_lh_template_average,\n",
    "                        area_data_lh_template_sum, curve_data_lh_template_sum, thickness_data_lh_template_sum, volumn_data_lh_template_sum])\n",
    "    df_lh.index = ['area_avg', 'curve_avg', 'thickness_avg', 'volume_avg','area_sum', 'curve_sum', 'thickness_sum', 'volume_sum']\n",
    "    \n",
    "    \n",
    "    df_rh = pd.DataFrame([area_data_rh_template_average, curve_data_rh_template_average, thickness_data_rh_template_average, volumn_data_rh_template_average,\n",
    "                        area_data_rh_template_sum, curve_data_rh_template_sum, thickness_data_rh_template_sum, volumn_data_rh_template_sum])\n",
    "    df_rh.index = ['area_avg', 'curve_avg', 'thickness_avg', 'volume_avg','area_sum', 'curve_sum', 'thickness_sum', 'volume_sum']\n",
    "    \n",
    "    \n",
    "    # print(df_lh)\n",
    "    # print(df_rh)\n",
    "    \n",
    "    return df_lh, df_rh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5236b-dfb3-44e2-8153-d46614e57a12",
   "metadata": {},
   "source": [
    "# Main Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea62f9b-5766-470b-bef2-5088887006fc",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the project ID\n",
    "# project_id = dxpy.api.user_get_project({'project': dxpy.DXProject('').get_id()})['project']\n",
    "# project_name = dxpy.DXProject(project_id).describe()['name']\n",
    "project_id = 'project-GxqpVq0Jpp5Py82xVbZV198y'  # Replace with the actual project ID\n",
    "\n",
    "# Create a DXProject object for the project\n",
    "project = dxpy.DXProject(project_id)\n",
    "\n",
    "# List the contents of the root folder\n",
    "folder_path = '/Bulk/Brain MRI/T1/'\n",
    "folder_contents = project.list_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6b9aa-b8d6-4ea1-91d7-c9ea02c44fe9",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load subject csv file\n",
    "command = f'dx download \"{project_id}:/{phenotype_origin_folder}/subject_list_annot_existed_latest.csv\"'\n",
    "subprocess.run(command, shell=True, check=True)\n",
    "df_annot_list = pd.read_csv('subject_list_annot_existed_latest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42867c7-f471-4b81-b0a7-263c2c1335d2",
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List the contents of the root folder\n",
    "annot_folder_path = f'/{phenotype_origin_folder}/Annotate_files'\n",
    "annot_folder_contents = project.list_folder(annot_folder_path)\n",
    "\n",
    "#########################\n",
    "# df_subj = []#columns = ['folder', 'Name','type','version','prefix']\n",
    "#########################\n",
    "\n",
    "for item in folder_contents['folders']:#for each folder\n",
    "    #List folders\n",
    "    # print(item)\n",
    "    folder_name = item.split('/')\n",
    "    \n",
    "        \n",
    "    ## folder create\n",
    "    command_path_check = f'dx ls \"{project_id}:/{phenotype_origin_folder}/Measurement_template/{folder_name[-1]}\"'\n",
    "    try:\n",
    "        subprocess.run(command_path_check, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    except subprocess.CalledProcessError:\n",
    "        command_path_check_create = f'dx mkdir \"{project_id}:/{phenotype_origin_folder}/Measurement_template/{folder_name[-1]}\"'\n",
    "        subprocess.run(command_path_check_create, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    uploading_path = project_id+':/'+phenotype_origin_folder+'/Measurement_template/'+folder_name[-1]+'/'\n",
    "\n",
    "    \n",
    "    # List files\n",
    "    files = dxpy.find_data_objects(classname='file', folder=item, project=project_id)\n",
    "    # print(files)\n",
    "    \n",
    "    # Missing annotate files\n",
    "    df_subj_annot = []\n",
    "    \n",
    "    for file in files:#for each file\n",
    "        ### Check\n",
    "        # print(file)\n",
    "        # print(file[\"id\"])\n",
    "                \n",
    "        dx_file = dxpy.DXFile(file[\"id\"])\n",
    "        file_metadata = dx_file.describe()\n",
    "        # print(f\"File Name: {file_metadata['name']}\")\n",
    "        # print(f\"File Size: {file_metadata['size']} bytes\")\n",
    "        # print(f\"File ID: {file_metadata['id']}\")\n",
    "\n",
    "        if '_20263_' in file_metadata['name']:#According to the file-name, only extract freesurfer files\n",
    "            print(f\"File Name: {file_metadata['name']}\")\n",
    "            # print(f\"File Size: {file_metadata['size']} bytes\")\n",
    "            # print(f\"File ID: {file_metadata['id']}\")\n",
    "            \n",
    "            # if file_metadata[\"name\"][:-4] == '2720263_20263_2_0':\n",
    "            \n",
    "            \n",
    "            ##### examplesï¼šload the existed check point         \n",
    "            #######################################\n",
    "            #Making the subject csv\n",
    "            name_list = file_metadata[\"name\"][:-4].split('_')\n",
    "\n",
    "            ######################################     \n",
    "\n",
    "            # Construct the download command\n",
    "            # print( project_id+\":\"+item+'/'+file_metadata['id'] )\n",
    "            # print(f\"File Name: {file_metadata['name']}\")\n",
    "\n",
    "            download_command = f'dx download \"{project_id}:{item}/{file_metadata[\"id\"]}\" -o {saving_dir}'\n",
    "            print(download_command)\n",
    "            subprocess.run(download_command, shell=True, check=True)\n",
    "\n",
    "            with zipfile.ZipFile(f\"{saving_dir}/{file_metadata['name']}\", 'r') as zip_ref:\n",
    "                zip_ref.extractall(saving_dir)\n",
    "\n",
    "            # Rename the folder\n",
    "            tmp_file_name = saving_dir+'/'+file_metadata['name'][:-4]\n",
    "            os.rename(saving_dir+'/FreeSurfer', saving_dir+'/'+file_metadata['name'][:-4])\n",
    "\n",
    "\n",
    "            # Remove zip files\n",
    "            os.remove(saving_dir+'/'+file_metadata[\"name\"])\n",
    "            # print(f\"'{file_metadata['name']}' file has been deleted.\")\n",
    "        \n",
    "\n",
    "            ##### make sure the folder is not empty:\n",
    "            if os.path.getsize(saving_dir+'/'+file_metadata['name'][:-4]+'/surf/') != 0:\n",
    "\n",
    "                ### file name from annotate\n",
    "                tmp_check_file_name = folder_name[-1]+'_'+file_metadata['name'][:-4]\n",
    "                # print(tmp_check_file_name)\n",
    "\n",
    "\n",
    "                ##### Check exists annotate files\n",
    "\n",
    "                check_lh = df_annot_list[(df_annot_list['name'].str.contains(file_metadata['name'][:-4])) & (df_annot_list['hemis'] == 'lh')]\n",
    "                check_rh = df_annot_list[(df_annot_list['name'].str.contains(file_metadata['name'][:-4])) & (df_annot_list['hemis'] == 'rh')]\n",
    "\n",
    "\n",
    "                if check_lh.empty or check_rh.empty:\n",
    "\n",
    "                    # template annotate generation\n",
    "                    command_annot2annot_lh, command_annot2annot_rh = surf_transform(file_metadata['name'][:-4],saving_dir,freesurfer_path)\n",
    "                    subprocess.run(command_annot2annot_lh)\n",
    "                    subprocess.run(command_annot2annot_rh)\n",
    "\n",
    "\n",
    "                    # Rename the annot file name\n",
    "                    tmp_file_name = saving_dir+'/'+file_metadata['name'][:-4]+'_lh.annot'\n",
    "                    os.rename(tmp_file_name, saving_dir+'/'+file_metadata['name'][0:2]+'_'+file_metadata['name'][:-4]+'_lh.annot')\n",
    "\n",
    "                    tmp_file_name = saving_dir+'/'+file_metadata['name'][:-4]+'_rh.annot'\n",
    "                    os.rename(tmp_file_name, saving_dir+'/'+file_metadata['name'][0:2]+'_'+file_metadata['name'][:-4]+'_rh.annot')\n",
    "\n",
    "                    # Extract measurements\n",
    "                    data_lh, data_rh = data_loading(saving_dir, file_metadata['name'][:-4])\n",
    "\n",
    "\n",
    "                    # Save the files\n",
    "                    data_lh.to_csv(saving_dir+'/'+tmp_check_file_name+'_Template_lh.csv', index=True)\n",
    "                    data_rh.to_csv(saving_dir+'/'+tmp_check_file_name+'_Template_rh.csv', index=True)\n",
    "\n",
    "                    ##################################\n",
    "                    #### upload files\n",
    "                    # Upload csv\n",
    "                    upload_command = f'dx upload {saving_dir}/{tmp_check_file_name}_Template_lh.csv -o {uploading_path}{tmp_check_file_name}_Template_lh.csv'\n",
    "                    # print(upload_command)\n",
    "                    subprocess.run(upload_command, shell=True, check=True) \n",
    "\n",
    "                    upload_command = f'dx upload {saving_dir}/{tmp_check_file_name}_Template_rh.csv -o {uploading_path}{tmp_check_file_name}_Template_rh.csv'\n",
    "                    # print(upload_command)\n",
    "                    subprocess.run(upload_command, shell=True, check=True) \n",
    "\n",
    "                    #Upload annot files\n",
    "                    upload_command = f'dx upload {saving_dir}/{tmp_check_file_name}_lh.annot -o {uploading_path}{tmp_check_file_name}_lh.annot'\n",
    "                    # print(upload_command)\n",
    "                    subprocess.run(upload_command, shell=True, check=True) \n",
    "\n",
    "                    upload_command = f'dx upload {saving_dir}/{tmp_check_file_name}_rh.annot -o {uploading_path}{tmp_check_file_name}_rh.annot'\n",
    "                    # print(upload_command)\n",
    "                    subprocess.run(upload_command, shell=True, check=True) \n",
    "\n",
    "                    ##################################\n",
    "\n",
    "                    # Remove annot files\n",
    "                    os.remove(saving_dir+'/'+tmp_check_file_name+'_lh.annot')\n",
    "                    os.remove(saving_dir+'/'+tmp_check_file_name+'_rh.annot')\n",
    "                    # print(f\"'{file_metadata['name'][:-4]}' lh.annot and rh.annot has been deleted.\")\n",
    "\n",
    "                    # Remove csv files\n",
    "                    os.remove(saving_dir+'/'+tmp_check_file_name+'_Template_lh.csv')\n",
    "                    os.remove(saving_dir+'/'+tmp_check_file_name+'_Template_rh.csv')\n",
    "                    # print(f\"'{file_metadata['name'][:-4]}' lh.annot and rh.annot has been deleted.\")\n",
    "\n",
    "                    df_subj_annot.append( [folder_name[-1], name_list[0], name_list[1], name_list[2], name_list[3]] )\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "\n",
    "                    #first one, ignore repeat ones\n",
    "                    check_lh_id = check_lh.iloc[0]\n",
    "                    check_rh_id = check_rh.iloc[0]\n",
    "\n",
    "                    download_command = f'dx download \"{project_id}:/{phenotype_origin_folder}/Annotate_files/{check_lh_id[\"file_id\"]}\" -o {saving_dir}'\n",
    "                    print(download_command)\n",
    "                    subprocess.run(download_command, shell=True, check=True) \n",
    "\n",
    "                    download_command = f'dx download \"{project_id}:/{phenotype_origin_folder}/Annotate_files/{check_rh_id[\"file_id\"]}\" -o {saving_dir}'\n",
    "                    print(download_command)\n",
    "                    subprocess.run(download_command, shell=True, check=True) \n",
    "\n",
    "                    data_lh, data_rh = data_loading(saving_dir, file_metadata['name'][:-4])\n",
    "\n",
    "                    # Save the files\n",
    "                    data_lh.to_csv(saving_dir+'/'+tmp_check_file_name+'_Template_lh.csv', index=True)\n",
    "                    data_rh.to_csv(saving_dir+'/'+tmp_check_file_name+'_Template_rh.csv', index=True)\n",
    "\n",
    "                    ##################################\n",
    "                    #### upload files\n",
    "                    # Upload csv\n",
    "                    upload_command = f'dx upload {saving_dir}/{tmp_check_file_name}_Template_lh.csv -o {uploading_path}{tmp_check_file_name}_Template_lh.csv'\n",
    "                    # print(upload_command)\n",
    "                    subprocess.run(upload_command, shell=True, check=True) \n",
    "\n",
    "                    upload_command = f'dx upload {saving_dir}/{tmp_check_file_name}_Template_rh.csv -o {uploading_path}{tmp_check_file_name}_Template_rh.csv'\n",
    "                    # print(upload_command)\n",
    "                    subprocess.run(upload_command, shell=True, check=True) \n",
    "\n",
    "                    #Upload annot files\n",
    "                    upload_command = f'dx upload {saving_dir}/{tmp_check_file_name}_lh.annot -o {uploading_path}{tmp_check_file_name}_lh.annot'\n",
    "                    # print(upload_command)\n",
    "                    subprocess.run(upload_command, shell=True, check=True) \n",
    "\n",
    "                    upload_command = f'dx upload {saving_dir}/{tmp_check_file_name}_rh.annot -o {uploading_path}{tmp_check_file_name}_rh.annot'\n",
    "                    # print(upload_command)\n",
    "                    subprocess.run(upload_command, shell=True, check=True) \n",
    "\n",
    "                    ##################################\n",
    "\n",
    "\n",
    "                    # Remove annot files\n",
    "                    os.remove(saving_dir+'/'+tmp_check_file_name+'_lh.annot')\n",
    "                    os.remove(saving_dir+'/'+tmp_check_file_name+'_rh.annot')\n",
    "                    # print(f\"'{file_metadata['name'][:-4]}' lh.annot and rh.annot has been deleted.\")\n",
    "\n",
    "                    # Remove csv files\n",
    "                    os.remove(saving_dir+'/'+tmp_check_file_name+'_Template_lh.csv')\n",
    "                    os.remove(saving_dir+'/'+tmp_check_file_name+'_Template_rh.csv')\n",
    "                    # print(f\"'{file_metadata['name'][:-4]}' lh.annot and rh.annot has been deleted.\")\n",
    "\n",
    "\n",
    "\n",
    "            #remove \n",
    "            # Delete zip and folder file\n",
    "            shutil.rmtree(saving_dir+'/'+file_metadata['name'][:-4])\n",
    "            # print(f\"'{file_metadata['name'][:-4]}' folder has been deleted.\")\n",
    "                \n",
    "        \n",
    "    ## Save the files\n",
    "    miss_subj = pd.DataFrame(df_subj_annot)\n",
    "    miss_subj.to_csv(saving_dir+'/'+'miss_subj.csv', index=True)   \n",
    "\n",
    "    upload_command = f'dx upload {saving_dir}/miss_subj.csv -o {uploading_path}miss_subj.csv'\n",
    "    print(upload_command)\n",
    "    subprocess.run(upload_command, shell=True, check=True) \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

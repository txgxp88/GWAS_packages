{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719caaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import posixpath\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download command\n",
    "def dx_download(project_name, file_name, dest_name):\n",
    "    quoted_path = f'\"{project_name}:{file_name}\"'\n",
    "    download_command = f\"dx download {quoted_path} -o {dest_name}\"\n",
    "    #print(f\"Downloading {file_name} from {project_name} to {dest_name}\")\n",
    "    subprocess.run(download_command, shell=True, check=True)\n",
    "\n",
    "# upload command\n",
    "def dx_upload(file_name, dest_name):\n",
    "    upload_command = f\"dx upload {file_name} -o {dest_name}\"\n",
    "    #print(f\"Uploading {file_name} to {dest_name}\")\n",
    "    subprocess.run(upload_command, shell=True, check=True)\n",
    "\n",
    "# directory list command\n",
    "def dx_dir_list(project_name, folder_name):\n",
    "    dx_path = f\"{project_name}:{folder_name}\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"dx\", \"ls\", dx_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            check=True,\n",
    "            text=True\n",
    "        )\n",
    "        return result.stdout.strip().split('\\n')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to list {dx_path}: {e.stderr}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d773985",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b452e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project name    \n",
    "# project_id = dxpy.api.user_get_project({'project': dxpy.DXProject('').get_id()})['project']\n",
    "# project_name = dxpy.DXProject(project_id).describe()['name']\n",
    "project_name = \"project-GxqpVq0Jpp5Py82xVbZV198y\"\n",
    "\n",
    "# cloud genotype data path\n",
    "genotype_origin_folder = \"/GWAS_pipeline\"\n",
    "\n",
    "\n",
    "# Create a directory for storing the downloaded files\n",
    "In_saving_folder = \"pheno\"\n",
    "if not os.path.exists(In_saving_folder):\n",
    "    os.makedirs(In_saving_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5eebc",
   "metadata": {},
   "source": [
    "## Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0894218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to collect all pheno files for all cohort images\n",
    "# Single file download task\n",
    "def collect_single_file(project_name, full_path, dest_path):\n",
    "    if os.path.exists(dest_path):\n",
    "        return \n",
    "    try:\n",
    "        dx_download(project_name, full_path, dest_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {full_path}: {e}\")\n",
    "\n",
    "def collect_pheno(project_name, folder_name, dest_folder, max_workers=12):\n",
    "    # Check all folders (10~59, will be update)\n",
    "    subfolder_list = dx_dir_list(project_name, folder_name)\n",
    "\n",
    "    # Creat task\n",
    "    tasks = []\n",
    "\n",
    "    for subfolder in subfolder_list:\n",
    "        subfolder_path = posixpath.join(folder_name, subfolder)\n",
    "        # Check all files in the subfolder\n",
    "        subfolder_files = dx_dir_list(project_name, subfolder_path)\n",
    "        for file in subfolder_files:\n",
    "            if file.endswith((\"Template_lh.csv\", \"Template_rh.csv\")):\n",
    "                full_path = posixpath.join(subfolder_path, file)\n",
    "                dest_path = os.path.join(dest_folder, file)\n",
    "                # Make it a task\n",
    "                tasks.append((project_name, full_path, dest_path))\n",
    "\n",
    "    # Run in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(collect_single_file, *task) for task in tasks]\n",
    "        concurrent.futures.wait(futures)\n",
    "        \n",
    "        \n",
    "# Collect the phenotype files\n",
    "phenotype_folder= \"Test/Measurement_template\"\n",
    "dest_folder = In_saving_folder\n",
    "collect_pheno(project_name, phenotype_folder, dest_folder, max_workers=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9e9f5",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the phenotype files\n",
    "# Read function return target format\n",
    "def read_pheno(file):\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "\n",
    "    df.index = df.index.astype(str)\n",
    "\n",
    "    # Extract the file name info\n",
    "    filename = os.path.basename(file)\n",
    "    parts = filename.split(\"_\")\n",
    "\n",
    "    try:\n",
    "        subject_id = parts[1]\n",
    "        version = f\"{parts[3]}_{parts[4]}\"\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"Filename {filename} is malformed.\")\n",
    "    fid = iid = subject_id\n",
    "    \n",
    "    # Remove medial wall\n",
    "    if \"-1\" in df.columns:\n",
    "        df = df.drop(columns=[\"-1\"])\n",
    "    # Add prefix to all remaining columns based on file name\n",
    "    prefix = \"lh\" if \"lh\" in file else \"rh\"\n",
    "\n",
    "    # Reformat the dataframe\n",
    "    # Extract per-vertex features (1~12)\n",
    "    vertices = [col for col in df.columns if col.isdigit()]\n",
    "    \n",
    "    out = {}\n",
    "\n",
    "    for parameter in df.index:\n",
    "        if parameter.endswith(\"_avg\"):\n",
    "            base = parameter.replace(\"_avg\", \"\")\n",
    "            out.update({\n",
    "                f\"{prefix}_{base}_{i}\": df.loc[parameter, v]\n",
    "                for i, v in enumerate(vertices, start=1)\n",
    "            })\n",
    "\n",
    "    # Add summary features\n",
    "    if \"area_sum\" in df.index:\n",
    "        out[f\"{prefix}_area_total\"] = df.loc[\"area_sum\", vertices].sum()\n",
    "    if \"thickness_sum\" in df.index:\n",
    "        out[f\"{prefix}_thickness_mean\"] = df.loc[\"thickness_sum\", vertices].mean()\n",
    "    if \"volume_sum\" in df.index:\n",
    "        out[f\"{prefix}_volume_total\"] = df.loc[\"volume_sum\", vertices].sum()\n",
    "\n",
    "    # create the final DataFrame\n",
    "    out = {\n",
    "        \"FID\": fid,\n",
    "        \"IID\": iid,\n",
    "        \"version\": version,\n",
    "        **out  # unsure, to be update\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(out, index=[0])\n",
    "\n",
    "\n",
    "\n",
    "# Make it parallel\n",
    "def parallel_read(files, max_workers=8):\n",
    "    results = []\n",
    "    total = len(files)\n",
    "    completed = 0\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(read_pheno, f): f for f in files}\n",
    "        for future in as_completed(future_to_file):\n",
    "            completed += 1\n",
    "            try:\n",
    "                df = future.result()\n",
    "                results.append(df)\n",
    "                print(f\"processed {completed}/{total} files\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {future_to_file[future]}: {e}\")\n",
    "    return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "# merge function for lh and rh data\n",
    "def merge_lh_rh(lh_files, rh_files, max_workers=8):\n",
    "    lh_df = parallel_read(lh_files, max_workers) if lh_files else pd.DataFrame()\n",
    "    rh_df = parallel_read(rh_files, max_workers) if rh_files else pd.DataFrame()\n",
    "\n",
    "    merged = pd.merge(lh_df, rh_df, on=[\"FID\", \"IID\", \"version\"], how=\"outer\")\n",
    "\n",
    "    id_cols = [\"FID\", \"IID\", \"version\"]\n",
    "    lh_cols = sorted([col for col in merged.columns if col.startswith(\"lh_\")])\n",
    "    rh_cols = sorted([col for col in merged.columns if col.startswith(\"rh_\")])\n",
    "    final_cols = id_cols + lh_cols + rh_cols\n",
    "    return merged[final_cols].sort_values(\"IID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "lh_files = glob.glob(\"pheno/*_Template_lh.csv\")\n",
    "rh_files = glob.glob(\"pheno/*_Template_rh.csv\")\n",
    "\n",
    "merged_df = merge_lh_rh(lh_files, rh_files)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the merged file\n",
    "merged_file_name = \"merged_pheno.csv\"\n",
    "merged_df.to_csv(merged_file_name, index=False)\n",
    "# Upload the merged file to the project\n",
    "file_name = \"merged_pheno.csv\"\n",
    "dest_name = f\"{genotype_origin_folder}/Image_dataset/merged_pheno.csv\"\n",
    "dx_upload(file_name, dest_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e023b",
   "metadata": {},
   "source": [
    "## QC: remove the outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea3efe",
   "metadata": {},
   "source": [
    "## Update subject list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select required columns from merged phenotype data\n",
    "merged_pheno = merged_df\n",
    "selected_columns = [\"FID\", \"IID\", \"version\"]\n",
    "lh_area_cols = [col for col in merged_pheno.columns if col.startswith(\"lh_area_\")]\n",
    "rh_area_cols = [col for col in merged_pheno.columns if col.startswith(\"rh_area_\")]\n",
    "lh_thick_cols = [col for col in merged_pheno.columns if col.startswith(\"lh_thickness_\")]\n",
    "rh_thick_cols = [col for col in merged_pheno.columns if col.startswith(\"rh_thickness_\")]\n",
    "volume_cols = [col for col in merged_pheno.columns if col.endswith(\"volume_total\")]\n",
    "\n",
    "final_selected_columns = selected_columns + lh_area_cols + rh_area_cols + lh_thick_cols + rh_thick_cols + volume_cols\n",
    "# Select the columns from the merged phenotype data\n",
    "select_pheno = merged_pheno[final_selected_columns]\n",
    "print(select_pheno.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final pheno data for mergeing lh and rh\n",
    "merged_df = select_pheno.copy()\n",
    "# Collect matching column pairs\n",
    "lh_cols = [col for col in merged_df.columns if col.startswith(\"lh_\") and col.replace(\"lh_\", \"rh_\") in merged_df.columns]\n",
    "# Create average columns\n",
    "for lh_col in lh_cols:\n",
    "    rh_col = lh_col.replace(\"lh_\", \"rh_\")\n",
    "    \n",
    "    if \"thickness\" in lh_col:\n",
    "        mean_col = lh_col.replace(\"lh_\", \"\")\n",
    "        merged_df[mean_col] = merged_df[[lh_col, rh_col]].mean(axis=1)\n",
    "    \n",
    "    elif \"area\" in lh_col:\n",
    "        sum_col = lh_col.replace(\"lh_\", \"\")\n",
    "        merged_df[sum_col] = merged_df[[lh_col, rh_col]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns starting with 'lh_' or 'rh_'\n",
    "cols_to_drop = [col for col in merged_df.columns if col.startswith(\"lh_\") or col.startswith(\"rh_\")]\n",
    "final_pheno = merged_df.drop(columns=cols_to_drop)\n",
    "# print(final_pheno.head())\n",
    "# print(list(final_pheno.columns))\n",
    "\n",
    "\n",
    "# Save the final phenotype data\n",
    "final_pheno.to_csv('final_pheno.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1580eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Clean the column names\n",
    "id_cols = ['FID', 'IID', 'version']\n",
    "# get all area and thickness region columns, excluding totals\n",
    "area_cols = sorted(\n",
    "    [col for col in final_pheno.columns if re.match(r'area_\\d+$', col)],\n",
    "    key=lambda x: int(x.split('_')[1])\n",
    ")\n",
    "thickness_cols = sorted(\n",
    "    [col for col in final_pheno.columns if re.match(r'thickness_\\d+$', col)],\n",
    "    key=lambda x: int(x.split('_')[1])\n",
    ")\n",
    "# Rename the total columns\n",
    "final_pheno_cleaned = final_pheno.rename(columns={\n",
    "    'area_total': 'total_SA',\n",
    "    'thickness_mean': 'mean_CT'\n",
    "})\n",
    "# Append the renamed summary columns at the end\n",
    "summary_cols = ['total_SA', 'mean_CT']\n",
    "# Final column order\n",
    "final_cols_order = id_cols + area_cols + thickness_cols + summary_cols\n",
    "final_pheno_cleaned = final_pheno_cleaned[final_cols_order]\n",
    "# Save the cleaned final phenotype data\n",
    "final_pheno_cleaned.to_csv('final_pheno_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4a7e7",
   "metadata": {},
   "source": [
    "## Filter subject list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count longitutinal participants\n",
    "\n",
    "version_counts = final_pheno_cleaned.groupby('IID')['version'].unique()\n",
    "revisit_counts = version_counts[version_counts.apply(lambda x: set(x) >= {'2_0', '3_0'})].index\n",
    "# Filter the phenotype data to remove participants with version 3_0 (keep only 2_0)\n",
    "final_pheno_subset = final_pheno_cleaned[final_pheno_cleaned['version'] != '3_0']\n",
    "\n",
    "print(f\"Participants with both 2_0 and 3_0: {len(revisit_counts)}\")\n",
    "print(f\"Original entries: {len(final_pheno_cleaned)}\")\n",
    "print(f\"Remaining entries after removing 3_0: {len(final_pheno_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce173a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the total surface area\n",
    "values = final_pheno_subset['total_SA']\n",
    "mean = values.mean()\n",
    "std = values.std()\n",
    "# Check num of outliers\n",
    "outliers = values[(values < mean - 3 * std) | (values > mean + 3 * std)]\n",
    "# print(f\"Number of outliers: {len(outliers)}\")\n",
    "\n",
    "\n",
    "# Remove outliers\n",
    "final_pheno_qced = final_pheno_subset[~final_pheno_subset['total_SA'].isin(outliers)]\n",
    "# print(f\"Remaining entries after removing outliers: {len(final_pheno_qced)}\")\n",
    "\n",
    "\n",
    "# filter the total thickness\n",
    "values = final_pheno_qced['mean_CT']\n",
    "mean = values.mean()\n",
    "std = values.std()\n",
    "# Check num of outliers\n",
    "outliers = values[(values < mean - 3 * std) | (values > mean + 3 * std)]\n",
    "# print(f\"Number of outliers: {len(outliers)}\")\n",
    "\n",
    "# Remove outliers\n",
    "final_pheno_qced = final_pheno_qced[~final_pheno_qced['mean_CT'].isin(outliers)]\n",
    "# print(f\"Remaining entries after removing outliers: {len(final_pheno_qced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82784ff4",
   "metadata": {},
   "source": [
    "### plot show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### plot show\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import norm\n",
    "\n",
    "# # Plot the histogram\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(values, bins=100, density=True, alpha=0.6, color='blue', edgecolor='black')\n",
    "\n",
    "# # Normal distribution curve\n",
    "# x = np.linspace(values.min(), values.max(), 1000)\n",
    "# pdf = norm.pdf(x, mean, std)\n",
    "# plt.plot(x, pdf, 'r--', label='Normal Distribution')\n",
    "\n",
    "# # Add outlier thresholds\n",
    "# plt.axvline(mean - 3 * std, color='red', linestyle='dashed', linewidth=1, label='-3 SD')\n",
    "# plt.axvline(mean + 3 * std, color='red', linestyle='dashed', linewidth=1, label='+3 SD')\n",
    "\n",
    "# plt.title('Histogram of Total Surface Area with Normal Curve')\n",
    "# plt.xlabel('Total Surface Area')\n",
    "# plt.ylabel('Density')\n",
    "# plt.grid(axis='y', alpha=0.75)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# # Remove outliers\n",
    "# final_pheno_qced = final_pheno_subset[~final_pheno_subset['total_SA'].isin(outliers)]\n",
    "# print(f\"Remaining entries after removing outliers: {len(final_pheno_qced)}\")\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import norm\n",
    "# # plot histogram of the total surface area\n",
    "# values = final_pheno_qced['mean_CT']\n",
    "# mean = values.mean()\n",
    "# std = values.std()\n",
    "# # Check num of outliers\n",
    "# outliers = values[(values < mean - 3 * std) | (values > mean + 3 * std)]\n",
    "# print(f\"Number of outliers: {len(outliers)}\")\n",
    "# # Plot the histogram\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(values, bins=100, density=True, alpha=0.6, color='blue', edgecolor='black')\n",
    "\n",
    "# # Normal distribution curve\n",
    "# x = np.linspace(values.min(), values.max(), 1000)\n",
    "# pdf = norm.pdf(x, mean, std)\n",
    "# plt.plot(x, pdf, 'r--', label='Normal Distribution')\n",
    "\n",
    "# # Add outlier thresholds\n",
    "# plt.axvline(mean - 3 * std, color='red', linestyle='dashed', linewidth=1, label='-3 SD')\n",
    "# plt.axvline(mean + 3 * std, color='red', linestyle='dashed', linewidth=1, label='+3 SD')\n",
    "\n",
    "# plt.title('Histogram of Mean Thickness with Normal Curve')\n",
    "# plt.xlabel('Total Surface Area')\n",
    "# plt.ylabel('Density')\n",
    "# plt.grid(axis='y', alpha=0.75)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# # Remove outliers\n",
    "# final_pheno_qced = final_pheno_qced[~final_pheno_qced['mean_CT'].isin(outliers)]\n",
    "# print(f\"Remaining entries after removing outliers: {len(final_pheno_qced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edf9aa",
   "metadata": {},
   "source": [
    "## Update all necesarry list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the full participant data\n",
    "\n",
    "dx_download(project_name, \"full_participant.csv\", \"full_participant.csv\")\n",
    "\n",
    "demographic_data = pd.read_csv('full_participant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Caucasian in demographic data\n",
    "caucasian_participants = demographic_data[demographic_data['p22006'] == 1.0]\n",
    "num_caucasian = (demographic_data['p22006'] == 1.0).sum()\n",
    "print(f\"Number of Caucasian participants: {num_caucasian}\")\n",
    "\n",
    "total_participants = len(demographic_data)\n",
    "print(f\"Total participants: {total_participants}\")\n",
    "print(f\"Proportion Caucasian: {100 * num_caucasian / total_participants:.2f}%\")\n",
    "\n",
    "# Check which participants in final_pheno_qced are in the Caucasian list\n",
    "caucasian_ids = set(caucasian_participants['eid'])\n",
    "matched_ids = final_pheno_qced[final_pheno_qced['IID'].isin(caucasian_ids)]\n",
    "\n",
    "print(f\"Total participants in final_pheno_qced: {len(final_pheno_qced)}\")\n",
    "print(f\"Number of Caucasian participants matched: {len(matched_ids)}\")\n",
    "print(f\"Percentage matched: {100 * len(matched_ids) / len(final_pheno_qced):.2f}%\")\n",
    "\n",
    "# Save the final phenotype data for Caucasian participants\n",
    "final_pheno_caucasian = final_pheno_qced[final_pheno_qced['IID'].isin(caucasian_ids)]\n",
    "print(f\"Final number of Caucasian participants: {len(final_pheno_caucasian)}\")\n",
    "\n",
    "# Save the cleaned final phenotype data\n",
    "final_pheno_caucasian.to_csv('final_pheno_caucasian.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e94bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate demographic data for fastGWA\n",
    "demographic_data = demographic_data.drop(columns=[\"p22006\"])\n",
    "# GCTA format\n",
    "demographic_data.insert(0, \"FID\", demographic_data[\"eid\"])\n",
    "demographic_data.insert(1, \"IID\", demographic_data[\"eid\"])\n",
    "# categorical variables\n",
    "covar_cols = [\"FID\", \"IID\", \"p31\", \"p54_i2\"]\n",
    "demographic_data[covar_cols].to_csv(\"ukb_covar.txt\", sep=\"\\t\", index=False, na_rep=\"NA\")\n",
    "# quantitative variables\n",
    "qcovar_cols = [\"FID\", \"IID\", \"p21003_i2\"] + [f\"p22009_a{i}\" for i in range(1, 11)]\n",
    "demographic_data[qcovar_cols].to_csv(\"ukb_qcovar.txt\", sep=\"\\t\", index=False, na_rep=\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e04831",
   "metadata": {},
   "source": [
    "## Update all necesarry list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload GRM files to DNA Nexus\n",
    "print(\"Uploading GRM files to DNA Nexus...\")\n",
    "dx_upload(\"final_pheno_caucasian.csv\", f\"/{genotype_origin_folder}/Image_dataset/final_pheno_caucasian.csv\")\n",
    "dx_upload(\"ukb_covar.txt\", f\"/{genotype_origin_folder}/Covariates/ukb_covar.txt\")\n",
    "dx_upload(\"ukb_qcovar.txt\", f\"/{genotype_origin_folder}/Covariates/ukb_qcovar.txt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
